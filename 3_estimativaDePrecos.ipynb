{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o exemplo da estimativa de preços de casas em Boston, usaremos as seguinte bibliotecas:\n",
    "- pandas e matplotlib para plotagem de resultados\n",
    "- xgboost e sklearn para modelos de regressão\n",
    "- sklearn.datasets para o conjunto de dados\n",
    "- numpy, math and random para geração de números aleatórios, funções matemáticas e manipulação de arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ignore errors not to polute the presentation\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#Introduce the basic package of data science.\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "#Introduce machine learning, preprocessing, model selection, and evaluation indicators.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "#Import the Boston dataset used this time.\n",
    "from sklearn.datasets import load_boston\n",
    "#Introduce algorithms.\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, LinearRegression, ElasticNet\n",
    "#Compared with SVC, it is the regression form of SVM.\n",
    "from sklearn.svm import SVR\n",
    "#Integrate algorithms.\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após a importação das bibliotecas, carregamos o conjunto de dados de trabalho e imprimimos os atributos dos imóveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Load the Boston house price data set.\n",
    "boston = load_boston()\n",
    "#x features, and y labels.\n",
    "x = boston.data\n",
    "y = boston.target\n",
    "#Display related attributes.\n",
    "print('Feature column name')\n",
    "print(boston.feature_names)\n",
    "print(\"Sample data volume: %d, number of features: %d\" % x.shape)\n",
    "print(\"Target sample data volume: %d\" % y.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A biblioteca Pandas pode ser então utilizada para processar os elementos e tabulá-los.\n",
    "- CRIM: urban per capita crime rate\n",
    "- ZN: proportion of residential land exceeds 25,000 square feet\n",
    "- INDUS: proportion of non-retail commercial land in a town\n",
    "- CHAS: Charles river empty variable (1 indicates that the boundary is a river; otherwise, the value is 0)\n",
    "- NOX: Nitric oxide concentration\n",
    "- RM: average number of rooms in a house\n",
    "- AGE: proportion of private houses completed before 1940\n",
    "- DIS: weighted distance to the five central regions of Boston\n",
    "- RAD: proximity index of a radial highway\n",
    "- TAX: full value property tax rate of USD 10,000\n",
    "- PTRATIO: proportion of teachers and students in urban areas\n",
    "- target: average price of private houses, unit: USD 1,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também podemos visualizar a distribuição dos preços destes imóveis (y) em milhares de dólares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=1)\n",
    "ax.hist(tuple(y), density=True, bins=20)\n",
    "pd.DataFrame(y).plot(kind='density', ax=ax)\n",
    "ax.set_xlabel(\"Average Price of Houses (x USD 1,000)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para treinar um modelo, podemos separar o conjunto de dados em dois conjuntos:\n",
    "- um para treinamento\n",
    "- outro para teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Segment the data.\n",
    "# 80% goes into the training set\n",
    "# 20% goes into the testing set\n",
    "# random_state is a shuffling seed (fixed for reproducible results)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=28)\n",
    "print(\"Before:\")\n",
    "x_train[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após a seleção dos conjuntos de treinamento e teste, precisamos fazer a normalização dos dados.\n",
    "\n",
    "CUIDADO: neste caso a normalização dos dados de teste é feita a partir dos valores médios e variância dos dados de treinamento\n",
    "\n",
    "Se a amostra de treinamento não for representativa, a modelagem terá problemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Store original dataset values before standardizing the data\n",
    "x_train_og = x_train\n",
    "x_test_og = x_test\n",
    "\n",
    "# Standardize the data set.\n",
    "# StandardScaler removes the mean and scales to unit variance.\n",
    "# z = (x - u) / s\n",
    "ss = StandardScaler()\n",
    "x_train = ss.fit_transform(x_train)  # Fit the data and transform to scale the training set\n",
    "x_test = ss.transform(x_test)  # Performs standardization on the test set\n",
    "# WARNING:\n",
    "# the ss.transform() standardization use the mean values and variance from the training set from ss.fit_transform()\n",
    "# if the mean and variance of the test set is dissimilar to the training set, bad things can happen\n",
    "print(\"After:\")\n",
    "x_train[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após preparados os conjuntos de dados para treinamento, podemos treinar modelos diversos.\n",
    "Para comparar o desempenho dos diferentes modelos, é utilizado o escore R2, que se aproxima de 1 quanto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Set the regression model name and instantiate the model\n",
    "regression_models = {\n",
    "    'LinerRegression': LinearRegression(),\n",
    "    'Ridge': RidgeCV(alphas=(0.001, 0.1, 1), cv=3),\n",
    "    'Lasso': LassoCV(alphas=(0.001, 0.1, 1), cv=5),\n",
    "    'Random Forrest': RandomForestRegressor(n_estimators=10),\n",
    "    'GBDT': GradientBoostingRegressor(n_estimators=30),\n",
    "    'Support Vector Regression': SVR(),\n",
    "    'ElasticNet': ElasticNet(alpha=0.001, max_iter=10000),\n",
    "    'XgBoost': XGBRegressor()\n",
    "}\n",
    "\n",
    "\n",
    "# cv is the cross-validation idea here.\n",
    "# Output the R2 scores of all regression models.\n",
    "# Define the R2 scoring function.\n",
    "def R2(model, x_train, x_test, y_train, y_test):\n",
    "    model_fitted = model.fit(x_train, y_train)\n",
    "    y_pred = model_fitted.predict(x_test)\n",
    "    score = r2_score(y_test, y_pred)\n",
    "    #print(\"Predito:\\n\", y_pred, \"\\nMedido:\\n\", y_test)\n",
    "    return score\n",
    "\n",
    "\n",
    "#Traverse all models to score.\n",
    "for (name, model) in regression_models.items():\n",
    "    score = R2(model, x_train, x_test, y_train, y_test)\n",
    "    print(\"{}: {:.6f}\".format(name, score.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E agora se a seleção dos dados para treinamento e teste não for bem feita\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_test_example = x_train_og[:10].loc[x_train_og[:10][\"ZN\"] > 0]\n",
    "x_train_example = x_train_og[:10].loc[x_train_og[:10][\"ZN\"] < 1]\n",
    "print(\"Before standardization\")\n",
    "print(\"xtraining\\n\", x_train_example, \"\\n\\n\", \"xtesting\\n\", x_test_example)\n",
    "\n",
    "y_test_example = [y_train[:10][4], y_train[:10][8]]\n",
    "y_train_example = [*y_train[:4], *y_train[5:8], *y_train[9:10]]\n",
    "#print(y_train[:10])\n",
    "#print(y_train_example)\n",
    "#print(y_test_example)\n",
    "ss1 = StandardScaler()\n",
    "x_train_example = ss1.fit_transform(x_train_example)  # Fit the data and transform to scale the training set\n",
    "x_test_example = ss1.transform(x_test_example)  # Performs standardization on the test set\n",
    "#print(\"\\n\\nAfter standardization\")\n",
    "#print(\"xtraining\\n\", x_train_example, \"\\n\\n\", \"xtesting\\n\", x_test_example)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "#Traverse all models to score.\n",
    "for (name, model) in regression_models.items():\n",
    "    score = R2(model, x_train_example, x_test_example, y_train_example, y_test_example)\n",
    "    print(\"{}: {:.6f}\".format(name, score.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como mostrado, a má seleção das amostras de teste podem fazer o modelo divergir, afastando-o do resultado esperado.\n",
    "\n",
    "De volta ao caso normal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Traverse all models to score.\n",
    "for (name, model) in regression_models.items():\n",
    "    score = R2(model, x_train, x_test, y_train, y_test)\n",
    "    print(\"{}: {:.6f}\".format(name, score.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como dito, o escore R2 se aproxima de 1 quanto mais próximos forem o resultado aproximado pelo modelo 'model.predict(x_test)' e o resultado medido 'y_test'\n",
    "Podemos visualizar o desempenho dos diferentes modelos de regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8), facecolor='w')\n",
    "##Perform visualization.\n",
    "ln_x_test = range(len(x_test))\n",
    "\n",
    "#Draw known prices in the test set\n",
    "plt.plot(ln_x_test, y_test, lw=4, label=u'Real prices in the test set')\n",
    "\n",
    "#Set legend, grid, plot title and limit x-axis range\n",
    "plt.grid(True)\n",
    "plt.title(u\"Boston Housing Price Forecast\")\n",
    "plt.xlim(0, 101)\n",
    "\n",
    "# Plot lines for each model prediction\n",
    "for (name, model) in regression_models.items():\n",
    "    y_predict = model.predict(x_test)\n",
    "    plt.plot(ln_x_test, y_predict, lw=2,\n",
    "             label=u'Predicted prices with %s, $R^2$=%.3f' % (name, r2_score(y_test, model.predict(x_test))))\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos escolher um dos modelos acima para tentar ajustar seus hiperparâmetros e melhorar o modelo.\n",
    "Neste caso, usaremos o SVR (Support Vector Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'kernel': ['linear', 'rbf', 'poly'],  # kernel function\n",
    "    'C': [0.1, 0.5, 0.9, 1, 5],  # SVR regularization factor\n",
    "    'gamma': [0.001, 0.01, 0.1, 1]\n",
    "    # 'rbf', 'poly' and 'sigmoid' kernel function coefficient, which affects the model performance\n",
    "}\n",
    "#Use grid search and perform cross validation.\n",
    "model = GridSearchCV(SVR(), param_grid=parameters, cv=3)\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O SVR aceita diferentes tipos de kernels, capazes de produzirem regressões mais adequadas a diferentes tipos de funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate SVR with different kernels\n",
    "svrs = {\"Linear\": SVR(kernel=\"linear\"),\n",
    "        \"Polynomial\": SVR(kernel=\"poly\"),\n",
    "        \"RBF\": SVR(kernel=\"rbf\"),\n",
    "        }\n",
    "\n",
    "\n",
    "# Create a dataset with 100 samples of a given function and split into test and train sets\n",
    "def dataset(func, samples=100):\n",
    "    x_train = list(range(samples))\n",
    "    y_train = list(map(lambda x: func(x), x_train))\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=28)\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "# Create 3 datasets with 2*x+noise, x^2-x*noise+noise, sin(x/10)\n",
    "datasets = {\n",
    "    \"Linear\": dataset(lambda x: x * 2 - random.gauss(0, 2)),\n",
    "    \"Polynomial\": dataset(lambda x: x ** 2 - x * random.gauss(0, 2) + random.gauss(0, 3)),\n",
    "    \"Sine\": dataset(lambda x: math.sin(x / 10)),\n",
    "}\n",
    "\n",
    "def test_svr_kernels(datasets):\n",
    "    # for each kernel and dataset, train the model, predict and plot the results in a grid\n",
    "    fig, axis = plt.subplots(nrows=3, sharex=True, figsize=(10, 15))\n",
    "\n",
    "    #Set legend, grid, plot title and limit x-axis range\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Test different SVR kernels with different datasets\")\n",
    "\n",
    "    i = 0\n",
    "    for (datasettype, dataset) in datasets.items():\n",
    "        x_train = dataset[0]\n",
    "        y_train = dataset[2]\n",
    "        x_test = dataset[1]\n",
    "        y_test = dataset[3]\n",
    "        axis[i].scatter(*list(zip(*sorted(zip(x_test, y_test)))), lw=2, label=u'Expected values')\n",
    "        axis[i].set_title(\"Dataset %s\" % datasettype)\n",
    "        # Plot lines for each model prediction\n",
    "        for (svrkernel, svr) in svrs.items():\n",
    "            svr.fit(numpy.array([x_train]).reshape(-1, 1), numpy.array([y_train]).reshape(-1, 1))\n",
    "            y_predict = svr.predict(numpy.array([x_test]).reshape(-1, 1))\n",
    "            score = r2_score(y_test, y_predict)\n",
    "\n",
    "            #*list(zip(*sorted(zip(x_test,y_test))))\n",
    "            # is a trick to merge the x and y lists into a single lists with (x,y) pairs,\n",
    "            # then sort them, and finally split back into separate lists of x and y\n",
    "            # If values passed to axis.plot or matplotlib.pyplot.plot are not sorted, incorrect lines will be drawn\n",
    "            axis[i].plot(*list(zip(*sorted(zip(x_test, y_predict)))), lw=2,\n",
    "                         label=u'Predicted with %s kernel, $R^2$=%.3f' % (svrkernel, score))\n",
    "            axis[i].legend(loc='lower right')\n",
    "        # Change to the next image\n",
    "        i += 1\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Visually show how different kernels fit to different data\n",
    "test_svr_kernels(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com as visualizações acima, podemos observar que diferentes kernels se comportam de maneiras distintas dependendo dos dados.\n",
    "Agora podemos ver o resultado da busca pelos melhores hiperparâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Optimal parameter list:\", model.best_params_)\n",
    "print(\"Optimal model:\", model.best_estimator_)\n",
    "print(\"Optimal R2 value:\", model.best_score_)\n",
    "print(\"R2 score with the test set:\", r2_score(y_test, model.predict(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontramos uma solução melhor que a inicial: escore R2 era de 0.51726 e foi para 0.61832\n",
    "\n",
    "Assim como visto nos plots anteriores, os preços oscilam e o kernel mais adequado do SVR é o rbf, que foi encontrado pela busca de hiperparâmetros.\n",
    "\n",
    "Quando satisfeitos com o resultado, podemos plotar e comparar visualmente os\n",
    "resultados esperados/medidos 'y_test' e os aproximados pelo modelo 'model.predict(x_test)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##Perform visualization.\n",
    "ln_x_test = range(len(x_test))\n",
    "y_predict = model.predict(x_test)\n",
    "#Set the canvas.\n",
    "plt.figure(figsize=(16, 8), facecolor='w')\n",
    "#Draw with a red solid line.\n",
    "plt.plot(ln_x_test, y_test, 'r-', lw=2, label=u'Value')\n",
    "#Draw with a green solid line.\n",
    "plt.plot(ln_x_test, y_predict, 'g-', lw=3, label=u'Estimated value of the SVR algorithm, $R^2$=%.3f' %\n",
    "                                                 r2_score(y_test, model.predict(x_test)))\n",
    "#Display in a diagram.\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.title(u\"Boston Housing Price Forecast (SVM)\")\n",
    "plt.xlim(0, 101)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E se aplicarmos o modelo sobre os dados de treinamento? Resultado do modelo é mais próximo do esperado, o que não\n",
    "quer dizer que o modelo é adequado, visto que pode estar viciado no conjunto de dados usado para treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##Perform visualization.\n",
    "ln_x_train = range(len(x_train))\n",
    "y_predict = model.predict(x_train)\n",
    "#Set the canvas.\n",
    "plt.figure(figsize=(16, 8), facecolor='w')\n",
    "#Draw with a red solid line.\n",
    "plt.plot(ln_x_train, y_train, 'r-', lw=2, label=u'Value')\n",
    "#Draw with a green solid line.\n",
    "plt.plot(ln_x_train, y_predict, 'g-', lw=3, label=u'Estimated value of the SVR algorithm, $R^2$=%.3f' %\n",
    "                                                  r2_score(y_train, model.predict(x_train)))\n",
    "#Display in a diagram.\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.title(u\"Boston Housing Price Forecast (SVM)\")\n",
    "plt.xlim(0, 101)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
